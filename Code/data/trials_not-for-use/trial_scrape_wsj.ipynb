{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the base URL for the Wall Street Journal news archive\n",
    "base_url = \"https://www.wsj.com/news/archive/\"\n",
    "\n",
    "# Set the start and end dates for the articles you want to scrape\n",
    "start_date = \"2020/01/01\"\n",
    "end_date = \"2020/01/30\"\n",
    "\n",
    "# Set the search term\n",
    "search_term = \"apple\"\n",
    "\n",
    "# Initialize an empty list to store the article titles\n",
    "article_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.wsj.com/news/archive/2020/01/01?page=1\n",
      "no articles\n"
     ]
    }
   ],
   "source": [
    "# Loop through each page of the news archive between the start and end dates\n",
    "for page_num in range(1, 101): # assuming there are no more than 100 pages of results\n",
    "    # Build the URL for the current page\n",
    "    url = f\"{base_url}{start_date}?page={page_num}\"\n",
    "\n",
    "    print(url)\n",
    "    \n",
    "    # Send a GET request to the URL and store the response\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content of the response using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all the article titles on the page that contain the search term\n",
    "    article_title_elements = soup.find_all(\"h2\", string=re.compile(search_term, re.IGNORECASE))\n",
    "    \n",
    "    # If no article titles are found on the page, break the loop\n",
    "    if len(article_title_elements) == 0:\n",
    "        print(\"no articles\")\n",
    "        break\n",
    "    \n",
    "    # Extract the text of each article title and add it to the list\n",
    "    for title_element in article_title_elements:\n",
    "        article_titles.append(title_element.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the list of article titles\n",
    "for title in article_titles:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping from  https://www.wsj.com/news/archive/2020/01/01\n",
      "<article class=\"WSJTheme--story--XB4V2mLz WSJTheme--full-width-image--3v8mC5L0 WSJTheme--media-margin-bottom--1bIRFuDR WSJTheme--error-page--JnBIdg_Z WSJTheme--no-overflow--2dsA4DmQ WSJTheme--margin-bottom-large--3j1dqUbC styles--margin-bottom-large--wa4U95NA\" data-id=\"WP-WSJ-0000786627\"><div class=\"WSJTheme--image--1RvJrX_o WSJTheme--media--2zsLCB98 WSJTheme--image-above--pBsXD1hr\" style=\"width:100%\"><a class=\"\" href=\"https://www.wsj.com/articles/young-people-avoid-facing-their-finances-more-than-ever-e853910e?mod=error_page\" tabindex=\"0\"><img alt=\"They Can’t Even: A Generation Avoids Facing Its Finances\" class=\"WSJTheme--image--At42misj\" height=\"191\" src=\"https://images.wsj.net/im-764141?width=287&amp;height=191\" srcset=\"https://images.wsj.net/im-764141?width=287&amp;height=191, https://images.wsj.net/im-764141?width=287&amp;height=191&amp;pixel_ratio=1.5 1.5x, https://images.wsj.net/im-764141?width=287&amp;height=191&amp;pixel_ratio=2 2x, https://images.wsj.net/im-764141?width=287&amp;height=191&amp;pixel_ratio=3 3x\" title=\"They Can’t Even: A Generation Avoids Facing Its Finances\" width=\"287\"/></a></div><div class=\"WSJTheme--articleType--34Gt-vdG\"> <span class=\"\" href=\"\">Personal Finance</span></div><div class=\"WSJTheme--headline--7VCzo7Ay\"><h2 class=\"WSJTheme--headline--unZqjb45 reset WSJTheme--heading-3--2z_phq5h typography--serif-display--ZXeuhS5E\"><a class=\"\" href=\"https://www.wsj.com/articles/young-people-avoid-facing-their-finances-more-than-ever-e853910e?mod=error_page\"><span class=\"WSJTheme--headlineText--He1ANr9C\">They Can’t Even: A Generation Avoids Facing Its Finances</span></a></h2></div></article>\n",
      "*****\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m*****\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[39m# check if the article headline contains the search keyword\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m headline \u001b[39m=\u001b[39m article\u001b[39m.\u001b[39;49mfind(\u001b[39m\"\u001b[39;49m\u001b[39mh3\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39mlower()\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m keyword \u001b[39min\u001b[39;00m headline:\n\u001b[1;32m     35\u001b[0m     \u001b[39m# extract the article link and add it to the list\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     link \u001b[39m=\u001b[39m article\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# set the start and end dates for the search\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2020, 1, 3)\n",
    "\n",
    "# set the search keyword\n",
    "keyword = \"hospitals\"\n",
    "\n",
    "# create an empty list to store the article headlines and links\n",
    "article_list = []\n",
    "\n",
    "# loop through each day between the start and end dates\n",
    "delta = timedelta(days=1)\n",
    "while start_date <= end_date:\n",
    "    # construct the URL for the WSJ archive page for the current date\n",
    "    date_str = start_date.strftime(\"%Y/%m/%d\")\n",
    "    url = f\"https://www.wsj.com/news/archive/{date_str}\"\n",
    "    print(\"Scraping from \", url)\n",
    "    \n",
    "    # send a GET request to the URL and parse the HTML content with Beautiful Soup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # find all the articles on the page and loop through them\n",
    "    articles = soup.find_all(\"article\")\n",
    "    for article in articles:\n",
    "        print(article)\n",
    "        print(\"*****\")\n",
    "        # check if the article headline contains the search keyword\n",
    "        headline = article.find(\"h3\").text.lower()\n",
    "        if keyword in headline:\n",
    "            # extract the article link and add it to the list\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            article_list.append((headline, link))\n",
    "    \n",
    "    # check if there is another page of results for the current date\n",
    "    next_page = soup.find(\"a\", {\"class\": \"page-link-next\"})\n",
    "    if next_page:\n",
    "        # if there is, update the URL and send another request\n",
    "        url = next_page[\"href\"]\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "    # move on to the next day\n",
    "    start_date += delta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPE WSJ NEWS ARCHIVE (WORKS!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "KEYWORD = 'zoom'\n",
    "\n",
    "# set the start and end dates for the search\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2020, 3, 30)\n",
    "\n",
    "# loop through each day between the start and end dates\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "headlines = []\n",
    "dates = []\n",
    "\n",
    "while start_date <= end_date:\n",
    "    \n",
    "    # construct the URL for the WSJ archive page for the current date\n",
    "    date_str = start_date.strftime(\"%Y/%m/%d\")\n",
    "\n",
    "    url = f\"https://www.wsj.com/news/archive/{date_str}\"\n",
    "    # print(\"Scraping from \", url)\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "    \n",
    "    # send a GET request to the URL and parse the HTML content with Beautiful Soup\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers).content, 'html.parser')\n",
    "\n",
    "    for article in soup.select('article'):\n",
    "        # print(article.span.text)\n",
    "        # print(article.h2.text)\n",
    "        # print(article.p.text)\n",
    "        # print('-' * 80)\n",
    "\n",
    "        headline = article.h2.text\n",
    "        date = article.p.text\n",
    "\n",
    "        if KEYWORD in headline.lower():\n",
    "            headlines.append(headline)\n",
    "            dates.append(date_str)\n",
    "\n",
    "    # move on to the next day\n",
    "    start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zoom Video Posts Fourth-Quarter Net Profit on 78% Revenue Growth', 'Zoom Video Buffered by Reality Check ', 'Slack Isn’t Zooming Yet       ', 'Zoom, CNBC and Jigsaw Puzzles: How America’s Shut-In Families Are Spending Their Days']\n"
     ]
    }
   ],
   "source": [
    "print(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020/03/04', '2020/03/04', '2020/03/12', '2020/03/21']\n"
     ]
    }
   ],
   "source": [
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url, KEYWORD, headlines, dates, date_str):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "    \n",
    "    # send a GET request to the URL and parse the HTML content with Beautiful Soup\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers).content, 'html.parser')\n",
    "\n",
    "    for article in soup.select('article'):\n",
    "        # print(article.span.text)\n",
    "        # print(article.h2.text)\n",
    "        # print(article.p.text)\n",
    "        # print('-' * 80)\n",
    "\n",
    "        headline = article.h2.text\n",
    "        # date = article.p.text\n",
    "\n",
    "        if KEYWORD in headline.lower():\n",
    "            headlines.append(headline)\n",
    "            dates.append(date_str)\n",
    "\n",
    "    return headlines, dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPE WSJ (OTHER PAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page(s):  2020/03/03 1\n",
      "Scraping page(s):  2020/03/03 2\n",
      "Scraping page(s):  2020/03/03 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/04 1\n",
      "Scraping page(s):  2020/03/04 2\n",
      "Scraping page(s):  2020/03/04 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/05 1\n",
      "Scraping page(s):  2020/03/05 2\n",
      "Scraping page(s):  2020/03/05 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/06 1\n",
      "Scraping page(s):  2020/03/06 2\n",
      "Scraping page(s):  2020/03/06 3\n",
      "Scraping page(s):  2020/03/06 4\n",
      "--------------------\n",
      "Scraping page:  2020/03/07 1\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/08 1\n",
      "Scraping page(s):  2020/03/08 2\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/09 1\n",
      "Scraping page(s):  2020/03/09 2\n",
      "Scraping page(s):  2020/03/09 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/10 1\n",
      "Scraping page(s):  2020/03/10 2\n",
      "Scraping page(s):  2020/03/10 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/11 1\n",
      "Scraping page(s):  2020/03/11 2\n",
      "Scraping page(s):  2020/03/11 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/12 1\n",
      "Scraping page(s):  2020/03/12 2\n",
      "Scraping page(s):  2020/03/12 3\n",
      "Scraping page(s):  2020/03/12 4\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/13 1\n",
      "Scraping page(s):  2020/03/13 2\n",
      "Scraping page(s):  2020/03/13 3\n",
      "--------------------\n",
      "Scraping page:  2020/03/14 1\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/15 1\n",
      "Scraping page(s):  2020/03/15 2\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/16 1\n",
      "Scraping page(s):  2020/03/16 2\n",
      "Scraping page(s):  2020/03/16 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/17 1\n",
      "Scraping page(s):  2020/03/17 2\n",
      "Scraping page(s):  2020/03/17 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/18 1\n",
      "Scraping page(s):  2020/03/18 2\n",
      "Scraping page(s):  2020/03/18 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/19 1\n",
      "Scraping page(s):  2020/03/19 2\n",
      "Scraping page(s):  2020/03/19 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/20 1\n",
      "Scraping page(s):  2020/03/20 2\n",
      "Scraping page(s):  2020/03/20 3\n",
      "--------------------\n",
      "Scraping page:  2020/03/21 1\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/22 1\n",
      "Scraping page(s):  2020/03/22 2\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/23 1\n",
      "Scraping page(s):  2020/03/23 2\n",
      "Scraping page(s):  2020/03/23 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/24 1\n",
      "Scraping page(s):  2020/03/24 2\n",
      "Scraping page(s):  2020/03/24 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/25 1\n",
      "Scraping page(s):  2020/03/25 2\n",
      "Scraping page(s):  2020/03/25 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/26 1\n",
      "Scraping page(s):  2020/03/26 2\n",
      "Scraping page(s):  2020/03/26 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/27 1\n",
      "Scraping page(s):  2020/03/27 2\n",
      "Scraping page(s):  2020/03/27 3\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/28 1\n",
      "Scraping page(s):  2020/03/28 2\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/29 1\n",
      "Scraping page(s):  2020/03/29 2\n",
      "--------------------\n",
      "Scraping page(s):  2020/03/30 1\n",
      "Scraping page(s):  2020/03/30 2\n",
      "Scraping page(s):  2020/03/30 3\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "KEYWORD = 'zoom'\n",
    "\n",
    "# set the start and end dates for the search\n",
    "start_date = datetime(2020, 3, 3)\n",
    "end_date = datetime(2020, 3, 30)\n",
    "\n",
    "# loop through each day between the start and end dates\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "headlines = []\n",
    "dates = []\n",
    "\n",
    "while start_date <= end_date:\n",
    "\n",
    "    # construct the URL for the WSJ archive page for the current date\n",
    "    date_str = start_date.strftime(\"%Y/%m/%d\")\n",
    "\n",
    "    # first page\n",
    "    page_num = 1\n",
    "\n",
    "    # check how many pages\n",
    "    url = f\"https://www.wsj.com/news/archive/{date_str}?page={page_num}\"\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    total_pages_element = driver.find_element(By.CSS_SELECTOR, 'span.WSJTheme--pagepicker-total--Kl350I1l')\n",
    "    total_pages_text = total_pages_element.text\n",
    "    \n",
    "    total_pages = int(total_pages_text.split()[-1])\n",
    "    # print(\"TEXT:\", total_pages)\n",
    "\n",
    "    if int(total_pages) > 1:\n",
    "        # print(\"Next page exists!\", date_str, page_num)\n",
    "\n",
    "        # scrape each page\n",
    "        while total_pages != 0:\n",
    "\n",
    "            # url scraper\n",
    "            url = f\"https://www.wsj.com/news/archive/{date_str}?page={page_num}\"\n",
    "\n",
    "            # scrape\n",
    "            # print(\"pages here!\", date_str, page_num)\n",
    "            print(\"Scraping page(s): \", date_str, page_num)\n",
    "            headlines, dates = scrape_page(url, KEYWORD, headlines, dates, date_str)\n",
    "\n",
    "            total_pages -= 1\n",
    "            page_num += 1\n",
    "            \n",
    "    else:\n",
    "        # print(\"Next page not found..\", date_str, page_num)\n",
    "        print(\"Scraping page: \", date_str, page_num)\n",
    "        headlines, dates = scrape_page(url, KEYWORD, headlines, dates, date_str)\n",
    "\n",
    "    # move on to the next day\n",
    "    start_date += delta\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zoom Video Posts Fourth-Quarter Net Profit on 78% Revenue Growth', 'Zoom Video Buffered by Reality Check ', 'Stocks to Watch: Guidewire Software, Zoom Video, United Airlines', 'Slack Isn’t Zooming Yet       ', 'Zoom, CNBC and Jigsaw Puzzles: How America’s Shut-In Families Are Spending Their Days', 'Who’s Zooming Whom?']\n"
     ]
    }
   ],
   "source": [
    "print(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020/03/04', '2020/03/04', '2020/03/05', '2020/03/12', '2020/03/21', '2020/03/27']\n"
     ]
    }
   ],
   "source": [
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combine the two lists into a DataFrame\n",
    "df = pd.DataFrame({'date': dates, 'news_headline': headlines})\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('zoom_stock_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb0dab884adde1e2026460bc948d2e1e6e2337a5a2ce1fe906357655ec0a50c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
