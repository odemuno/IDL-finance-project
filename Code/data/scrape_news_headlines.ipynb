{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape one WSJ page\n",
    "def scrape_page(url, KEYWORD, headlines, dates, date_str):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "    \n",
    "    # send a GET request to the URL and parse the HTML content with Beautiful Soup\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers).content, 'html.parser')\n",
    "\n",
    "    for article in soup.select('article'):\n",
    "        headline = article.h2.text\n",
    "        \n",
    "        if KEYWORD in headline.lower():\n",
    "            headlines.append(headline)\n",
    "            dates.append(date_str)\n",
    "\n",
    "    return headlines, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape multiple pages of a range of dates\n",
    "def scrape_news_headlines(start_date, end_date, delta, KEYWORD, headlines, dates, file_name):\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    while start_date <= end_date:\n",
    "\n",
    "        # construct the URL for the WSJ archive page for the current date\n",
    "        date_str = start_date.strftime(\"%Y/%m/%d\")\n",
    "\n",
    "        # first page\n",
    "        page_num = 1\n",
    "\n",
    "        # check how many pages\n",
    "        url = f\"https://www.wsj.com/news/archive/{date_str}?page={page_num}\"\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        total_pages_element = driver.find_element(By.CSS_SELECTOR, 'span.WSJTheme--pagepicker-total--Kl350I1l')\n",
    "        total_pages_text = total_pages_element.text\n",
    "        \n",
    "        total_pages = int(total_pages_text.split()[-1])\n",
    "\n",
    "        if int(total_pages) > 1:\n",
    "\n",
    "            # scrape each page\n",
    "            while total_pages != 0:\n",
    "\n",
    "                # url scraper\n",
    "                url = f\"https://www.wsj.com/news/archive/{date_str}?page={page_num}\"\n",
    "\n",
    "                # scrape\n",
    "                print(\"Scraping page(s): \", date_str, page_num)\n",
    "                headlines, dates = scrape_page(url, KEYWORD, headlines, dates, date_str)\n",
    "\n",
    "                total_pages -= 1\n",
    "                page_num += 1\n",
    "                \n",
    "        else:\n",
    "            # print(\"Next page not found..\", date_str, page_num)\n",
    "            print(\"Scraping page: \", date_str, page_num)\n",
    "            headlines, dates = scrape_page(url, KEYWORD, headlines, dates, date_str)\n",
    "\n",
    "        # move on to the next day\n",
    "        start_date += delta\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Combine the two lists into a DataFrame\n",
    "    df = pd.DataFrame({'date': dates, 'news_headline': headlines})\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "    return headlines, dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run below to scrape for individual stocks\n",
    "Change the keyword and the time duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page(s):  2020/03/04 1\n",
      "Scraping page(s):  2020/03/04 2\n",
      "Scraping page(s):  2020/03/04 3\n",
      "--------------------------------------------------\n",
      "Scraping page(s):  2020/03/05 1\n",
      "Scraping page(s):  2020/03/05 2\n",
      "Scraping page(s):  2020/03/05 3\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "KEYWORD = 'zoom'\n",
    "\n",
    "# set the start and end dates for the search\n",
    "start_date = datetime(2020, 3, 4)\n",
    "end_date = datetime(2020, 3, 5)\n",
    "\n",
    "# loop through each day between the start and end dates\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "headlines = []\n",
    "dates = []\n",
    "file_name = \"test_zoom_stock.csv\"\n",
    "\n",
    "# call function!\n",
    "headlines, dates = scrape_news_headlines(start_date, end_date, delta, KEYWORD, headlines, dates, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb0dab884adde1e2026460bc948d2e1e6e2337a5a2ce1fe906357655ec0a50c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
