{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the base URL for the Wall Street Journal news archive\n",
    "base_url = \"https://www.wsj.com/news/archive/\"\n",
    "\n",
    "# Set the start and end dates for the articles you want to scrape\n",
    "start_date = \"2020/01/01\"\n",
    "end_date = \"2020/01/30\"\n",
    "\n",
    "# Set the search term\n",
    "search_term = \"apple\"\n",
    "\n",
    "# Initialize an empty list to store the article titles\n",
    "article_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.wsj.com/news/archive/2020/01/01?page=1\n",
      "no articles\n"
     ]
    }
   ],
   "source": [
    "# Loop through each page of the news archive between the start and end dates\n",
    "for page_num in range(1, 101): # assuming there are no more than 100 pages of results\n",
    "    # Build the URL for the current page\n",
    "    url = f\"{base_url}{start_date}?page={page_num}\"\n",
    "\n",
    "    print(url)\n",
    "    \n",
    "    # Send a GET request to the URL and store the response\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content of the response using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all the article titles on the page that contain the search term\n",
    "    article_title_elements = soup.find_all(\"h2\", string=re.compile(search_term, re.IGNORECASE))\n",
    "    \n",
    "    # If no article titles are found on the page, break the loop\n",
    "    if len(article_title_elements) == 0:\n",
    "        print(\"no articles\")\n",
    "        break\n",
    "    \n",
    "    # Extract the text of each article title and add it to the list\n",
    "    for title_element in article_title_elements:\n",
    "        article_titles.append(title_element.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the list of article titles\n",
    "for title in article_titles:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping from  https://www.wsj.com/news/archive/2020/01/01\n",
      "<article class=\"WSJTheme--story--XB4V2mLz WSJTheme--full-width-image--3v8mC5L0 WSJTheme--media-margin-bottom--1bIRFuDR WSJTheme--error-page--JnBIdg_Z WSJTheme--no-overflow--2dsA4DmQ WSJTheme--margin-bottom-large--3j1dqUbC styles--margin-bottom-large--wa4U95NA\" data-id=\"WP-WSJ-0000786627\"><div class=\"WSJTheme--image--1RvJrX_o WSJTheme--media--2zsLCB98 WSJTheme--image-above--pBsXD1hr\" style=\"width:100%\"><a class=\"\" href=\"https://www.wsj.com/articles/young-people-avoid-facing-their-finances-more-than-ever-e853910e?mod=error_page\" tabindex=\"0\"><img alt=\"They Can’t Even: A Generation Avoids Facing Its Finances\" class=\"WSJTheme--image--At42misj\" height=\"191\" src=\"https://images.wsj.net/im-764141?width=287&amp;height=191\" srcset=\"https://images.wsj.net/im-764141?width=287&amp;height=191, https://images.wsj.net/im-764141?width=287&amp;height=191&amp;pixel_ratio=1.5 1.5x, https://images.wsj.net/im-764141?width=287&amp;height=191&amp;pixel_ratio=2 2x, https://images.wsj.net/im-764141?width=287&amp;height=191&amp;pixel_ratio=3 3x\" title=\"They Can’t Even: A Generation Avoids Facing Its Finances\" width=\"287\"/></a></div><div class=\"WSJTheme--articleType--34Gt-vdG\"> <span class=\"\" href=\"\">Personal Finance</span></div><div class=\"WSJTheme--headline--7VCzo7Ay\"><h2 class=\"WSJTheme--headline--unZqjb45 reset WSJTheme--heading-3--2z_phq5h typography--serif-display--ZXeuhS5E\"><a class=\"\" href=\"https://www.wsj.com/articles/young-people-avoid-facing-their-finances-more-than-ever-e853910e?mod=error_page\"><span class=\"WSJTheme--headlineText--He1ANr9C\">They Can’t Even: A Generation Avoids Facing Its Finances</span></a></h2></div></article>\n",
      "*****\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m*****\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[39m# check if the article headline contains the search keyword\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m headline \u001b[39m=\u001b[39m article\u001b[39m.\u001b[39;49mfind(\u001b[39m\"\u001b[39;49m\u001b[39mh3\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39mlower()\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m keyword \u001b[39min\u001b[39;00m headline:\n\u001b[1;32m     35\u001b[0m     \u001b[39m# extract the article link and add it to the list\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     link \u001b[39m=\u001b[39m article\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m\"\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# set the start and end dates for the search\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2020, 1, 3)\n",
    "\n",
    "# set the search keyword\n",
    "keyword = \"hospitals\"\n",
    "\n",
    "# create an empty list to store the article headlines and links\n",
    "article_list = []\n",
    "\n",
    "# loop through each day between the start and end dates\n",
    "delta = timedelta(days=1)\n",
    "while start_date <= end_date:\n",
    "    # construct the URL for the WSJ archive page for the current date\n",
    "    date_str = start_date.strftime(\"%Y/%m/%d\")\n",
    "    url = f\"https://www.wsj.com/news/archive/{date_str}\"\n",
    "    print(\"Scraping from \", url)\n",
    "    \n",
    "    # send a GET request to the URL and parse the HTML content with Beautiful Soup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # find all the articles on the page and loop through them\n",
    "    articles = soup.find_all(\"article\")\n",
    "    for article in articles:\n",
    "        print(article)\n",
    "        print(\"*****\")\n",
    "        # check if the article headline contains the search keyword\n",
    "        headline = article.find(\"h3\").text.lower()\n",
    "        if keyword in headline:\n",
    "            # extract the article link and add it to the list\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            article_list.append((headline, link))\n",
    "    \n",
    "    # check if there is another page of results for the current date\n",
    "    next_page = soup.find(\"a\", {\"class\": \"page-link-next\"})\n",
    "    if next_page:\n",
    "        # if there is, update the URL and send another request\n",
    "        url = next_page[\"href\"]\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "    # move on to the next day\n",
    "    start_date += delta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPE WSJ NEWS ARCHIVE (WORKS!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "KEYWORD = 'zoom'\n",
    "\n",
    "# set the start and end dates for the search\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2020, 3, 30)\n",
    "\n",
    "# loop through each day between the start and end dates\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "headlines = []\n",
    "dates = []\n",
    "\n",
    "while start_date <= end_date:\n",
    "    \n",
    "    # construct the URL for the WSJ archive page for the current date\n",
    "    date_str = start_date.strftime(\"%Y/%m/%d\")\n",
    "\n",
    "    url = f\"https://www.wsj.com/news/archive/{date_str}\"\n",
    "    # print(\"Scraping from \", url)\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "    \n",
    "    # send a GET request to the URL and parse the HTML content with Beautiful Soup\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers).content, 'html.parser')\n",
    "\n",
    "    for article in soup.select('article'):\n",
    "        # print(article.span.text)\n",
    "        # print(article.h2.text)\n",
    "        # print(article.p.text)\n",
    "        # print('-' * 80)\n",
    "\n",
    "        headline = article.h2.text\n",
    "        date = article.p.text\n",
    "\n",
    "        if KEYWORD in headline.lower():\n",
    "            headlines.append(headline)\n",
    "            dates.append(date_str)\n",
    "\n",
    "    # move on to the next day\n",
    "    start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zoom Video Posts Fourth-Quarter Net Profit on 78% Revenue Growth', 'Zoom Video Buffered by Reality Check ', 'Slack Isn’t Zooming Yet       ', 'Zoom, CNBC and Jigsaw Puzzles: How America’s Shut-In Families Are Spending Their Days']\n"
     ]
    }
   ],
   "source": [
    "print(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020/03/04', '2020/03/04', '2020/03/12', '2020/03/21']\n"
     ]
    }
   ],
   "source": [
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url, KEYWORD, headlines, dates):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "    \n",
    "    # send a GET request to the URL and parse the HTML content with Beautiful Soup\n",
    "    soup = BeautifulSoup(requests.get(url, headers=headers).content, 'html.parser')\n",
    "\n",
    "    for article in soup.select('article'):\n",
    "        # print(article.span.text)\n",
    "        # print(article.h2.text)\n",
    "        # print(article.p.text)\n",
    "        # print('-' * 80)\n",
    "\n",
    "        headline = article.h2.text\n",
    "        date = article.p.text\n",
    "\n",
    "        if KEYWORD in headline.lower():\n",
    "            headlines.append(headline)\n",
    "            dates.append(date_str)\n",
    "\n",
    "    return headlines, dates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPE WSJ (OTHER PAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: 1\n",
      "Next page not found.. 2020/01/01 1\n",
      "TEXT: 3\n",
      "Next page exists! 2020/01/02 1\n",
      "pages here! 2020/01/02 1\n",
      "pages here! 2020/01/02 2\n",
      "pages here! 2020/01/02 3\n",
      "TEXT: 3\n",
      "Next page exists! 2020/01/03 1\n",
      "pages here! 2020/01/03 1\n",
      "pages here! 2020/01/03 2\n",
      "pages here! 2020/01/03 3\n",
      "TEXT: 1\n",
      "Next page not found.. 2020/01/04 1\n",
      "TEXT: 2\n",
      "Next page exists! 2020/01/05 1\n",
      "pages here! 2020/01/05 1\n",
      "pages here! 2020/01/05 2\n",
      "TEXT: 2\n",
      "Next page exists! 2020/01/06 1\n",
      "pages here! 2020/01/06 1\n",
      "pages here! 2020/01/06 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m# check how many pages\u001b[39;00m\n\u001b[1;32m     32\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://www.wsj.com/news/archive/\u001b[39m\u001b[39m{\u001b[39;00mdate_str\u001b[39m}\u001b[39;00m\u001b[39m?page=\u001b[39m\u001b[39m{\u001b[39;00mpage_num\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m driver\u001b[39m.\u001b[39;49mget(url)\n\u001b[1;32m     36\u001b[0m total_pages_element \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mfind_element(By\u001b[39m.\u001b[39mCSS_SELECTOR, \u001b[39m'\u001b[39m\u001b[39mspan.WSJTheme--pagepicker-total--Kl350I1l\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m total_pages_text \u001b[39m=\u001b[39m total_pages_element\u001b[39m.\u001b[39mtext\n",
      "File \u001b[0;32m~/Documents/GitHub/CMU_SPR23/11785-deeplearning/IDL-finance-project/idlenv/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py:442\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m    Loads a web page in the current browser session.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m'\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m'\u001b[39;49m: url})\n",
      "File \u001b[0;32m~/Documents/GitHub/CMU_SPR23/11785-deeplearning/IDL-finance-project/idlenv/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py:428\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    425\u001b[0m         params[\u001b[39m'\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[1;32m    427\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_value(params)\n\u001b[0;32m--> 428\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[1;32m    429\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m    430\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/Documents/GitHub/CMU_SPR23/11785-deeplearning/IDL-finance-project/idlenv/lib/python3.8/site-packages/selenium/webdriver/remote/remote_connection.py:347\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    345\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[1;32m    346\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 347\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[0;32m~/Documents/GitHub/CMU_SPR23/11785-deeplearning/IDL-finance-project/idlenv/lib/python3.8/site-packages/selenium/webdriver/remote/remote_connection.py:369\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    366\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 369\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    370\u001b[0m     statuscode \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus\n\u001b[1;32m    371\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/CMU_SPR23/11785-deeplearning/IDL-finance-project/idlenv/lib/python3.8/site-packages/urllib3/request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39;49mfields, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/CMU_SPR23/11785-deeplearning/IDL-finance-project/idlenv/lib/python3.8/site-packages/urllib3/request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(headers)\n\u001b[1;32m    168\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
      "File \u001b[0;32m~/Documents/GitHub/CMU_SPR23/11785-deeplearning/IDL-finance-project/idlenv/lib/python3.8/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/Documents/GitHub/CMU_SPR23/11785-deeplearning/IDL-finance-project/idlenv/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/CMU_SPR23/11785-deeplearning/IDL-finance-project/idlenv/lib/python3.8/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/CMU_SPR23/11785-deeplearning/IDL-finance-project/idlenv/lib/python3.8/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py:1347\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1347\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1348\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1349\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py:307\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    308\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    309\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py:268\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 268\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    270\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "KEYWORD = 'zoom'\n",
    "\n",
    "# set the start and end dates for the search\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2020, 3, 30)\n",
    "\n",
    "# loop through each day between the start and end dates\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "headlines = []\n",
    "dates = []\n",
    "\n",
    "while start_date <= end_date:\n",
    "\n",
    "    # construct the URL for the WSJ archive page for the current date\n",
    "    date_str = start_date.strftime(\"%Y/%m/%d\")\n",
    "\n",
    "    # first page\n",
    "    page_num = 1\n",
    "\n",
    "    # check how many pages\n",
    "    url = f\"https://www.wsj.com/news/archive/{date_str}?page={page_num}\"\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    total_pages_element = driver.find_element(By.CSS_SELECTOR, 'span.WSJTheme--pagepicker-total--Kl350I1l')\n",
    "    total_pages_text = total_pages_element.text\n",
    "    \n",
    "    total_pages = int(total_pages_text.split()[-1])\n",
    "    print(\"TEXT:\", total_pages)\n",
    "\n",
    "    if int(total_pages) > 1:\n",
    "        print(\"Next page exists!\", date_str, page_num)\n",
    "\n",
    "        # scrape each page\n",
    "        while total_pages != 0:\n",
    "\n",
    "            # url scraper\n",
    "            url = f\"https://www.wsj.com/news/archive/{date_str}?page={page_num}\"\n",
    "\n",
    "            # scrape\n",
    "            # print(\"pages here!\", date_str, page_num)\n",
    "            headlines, dates = scrape_page(url, KEYWORD, headlines, dates)\n",
    "\n",
    "            total_pages -= 1\n",
    "            page_num += 1\n",
    "            \n",
    "    else:\n",
    "        # print(\"Next page not found..\", date_str, page_num)\n",
    "        headlines, dates = scrape_page(url, KEYWORD, headlines, dates)\n",
    "\n",
    "    # move on to the next day\n",
    "    start_date += delta\n",
    "\n",
    "\n",
    "    # while True: # loop through pages\n",
    "    \n",
    "    #     # construct the URL for the WSJ archive page for the current date\n",
    "    #     date_str = start_date.strftime(\"%Y/%m/%d\")\n",
    "\n",
    "    #     # https://www.wsj.com/news/archive/2020/01/02?page=1\n",
    "    #     url = f\"https://www.wsj.com/news/archive/{date_str}?page={page_num}\"\n",
    "    #     response = requests.get(url)\n",
    "    #     # print(response.status_code) #403\n",
    "    #     # print(\"Scraping from \", url)\n",
    "\n",
    "    #     ?\n",
    "\n",
    "    #     # if response.status_code == 403:\n",
    "    #     #     print(\"page_num: \" , page_num, date_str)\n",
    "\n",
    "    #     #     headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "            \n",
    "    #     #     # send a GET request to the URL and parse the HTML content with Beautiful Soup\n",
    "    #     #     soup = BeautifulSoup(requests.get(url, headers=headers).content, 'html.parser')\n",
    "\n",
    "    #     #     for article in soup.select('article'):\n",
    "    #     #         # print(article.span.text)\n",
    "    #     #         # print(article.h2.text)\n",
    "    #     #         # print(article.p.text)\n",
    "    #     #         # print('-' * 80)\n",
    "\n",
    "    #     #         headline = article.h2.text\n",
    "    #     #         date = article.p.text\n",
    "\n",
    "    #     #         if KEYWORD in headline.lower():\n",
    "    #     #             headlines.append(headline)\n",
    "    #     #             dates.append(date_str)\n",
    "\n",
    "    #     #     page_num += 1\n",
    "\n",
    "    #     # else:\n",
    "    #     #     page_num = 1\n",
    "    #     #     print(\"no other page for \", date_str)\n",
    "    #     #     break # go to another day instead od another page\n",
    "\n",
    "    # # move on to the next day\n",
    "    # start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb0dab884adde1e2026460bc948d2e1e6e2337a5a2ce1fe906357655ec0a50c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
