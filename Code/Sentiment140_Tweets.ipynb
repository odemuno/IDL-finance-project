{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "inOrkHvMlXS_"
      },
      "source": [
        "# Sentiment140: Predicting Stock Movement Using Sentiment Analysis of Twitter Feed with Neural Networks\n",
        "\n",
        "- baseline: https://www.scirp.org/journal/paperinformation.aspx?paperid=104142#ref9\n",
        "- sentiment kaggle: https://www.kaggle.com/datasets/kazanova/sentiment140 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2wOKRlh30DI"
      },
      "source": [
        "# Sentiment 140 Data\n",
        "For the training data, we are going to use a sentiment tagged Twitter dataset of 1.6 million tweets, collected from Sentiment140 for sentiment classification. The tweets are tagged ‘1’ and ‘0’ for being ‘positive’ and ‘negative’ respectively.\n",
        "\n",
        "It contains the following 6 fields:\n",
        "\n",
        "1. **target**: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
        "\n",
        "2. **ids**: The id of the tweet ( 2087)\n",
        "\n",
        "3. **date**: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "\n",
        "4. **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "\n",
        "5. **user**: the user that tweeted (robotickilldozr)\n",
        "\n",
        "6. **text**: the text of the tweet (Lyx is cool)\n",
        "\n",
        "Citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMSnqBKp5Fm5"
      },
      "source": [
        "### Download the data\n",
        "Download from the Kaggle website and then save to your local directory. \n",
        "\n",
        "Kaggle: https://www.kaggle.com/datasets/kazanova/sentiment140\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xapVlOt2XyxO"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RURsX5hXDP_h"
      },
      "outputs": [],
      "source": [
        "# utilities\n",
        "import string \n",
        "import re \n",
        "import pickle # not used\n",
        "import pandas as pd \n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ9rLm57lSw1"
      },
      "outputs": [],
      "source": [
        "# nltk\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt') \n",
        "nltk.download('wordnet') \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqY_OlIsPbJN"
      },
      "outputs": [],
      "source": [
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2A4rqssgPAa"
      },
      "source": [
        "### Load in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfpHqvto5Ofo"
      },
      "outputs": [],
      "source": [
        "# import drive so you can access your folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw5FWRfFgJPK"
      },
      "outputs": [],
      "source": [
        "# read in data\n",
        "df = pd.read_csv('PATH_TO_training.1600000.processed.noemoticon.csv', encoding='latin')\n",
        "\n",
        "# add in the column names\n",
        "df.columns = ['sentiment', 'tweet_id', 'time', 'flag', 'user', 'tweet']\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ei00dwFhVcK"
      },
      "outputs": [],
      "source": [
        "# create a new dataframe with only tweets and sentiment\n",
        "features ='tweet'\n",
        "target = 'sentiment'\n",
        "\n",
        "df = df[[features, target]]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH4792XTh8L6"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uo80AS8h_ji"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNUHua2EiVmA"
      },
      "outputs": [],
      "source": [
        "# downcast to smaller integer size to reduce memory: int64 -> int8\n",
        "df['sentiment'] = pd.to_numeric(df['sentiment'], downcast='integer')\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLfR5-7SiyUU"
      },
      "outputs": [],
      "source": [
        "# find all sentiment values in the dataset \n",
        "df.sentiment.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZ5xUMuOi5w6"
      },
      "outputs": [],
      "source": [
        "# change 4 to 1 (positive)\n",
        "df['sentiment'] = df['sentiment'].replace(4, 1)\n",
        "df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRM9gYcYC2tw"
      },
      "outputs": [],
      "source": [
        "class TweetCleaner:\n",
        "  def __init__(self):\n",
        "    self.stop_words = set(stopwords.words('english'))\n",
        "    self.emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':(': 'sad', 'XD': 'laughing',\n",
        "          ':-(': 'sad', ':-<': 'sad', ':P': 'stuck-out-tongue', ':O': 'surprised',\n",
        "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
        "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
        "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
        "          ':/': 'confused', ':|': 'neutral-face', \":'-)\": 'sadsmile', \"<3\": 'love',\n",
        "          \":'-)\": 'tears-of-happiness'}\n",
        "\n",
        "  def lowercase(self, tweet):\n",
        "    ''' Each text is converted to lowercase. '''\n",
        "    return tweet.lower()\n",
        "  \n",
        "  def replace_url(self, tweet):\n",
        "    ''' Links starting with “Http” or “https” or “www” are replaced by “URL” '''\n",
        "    url_regex = re.compile(r'(http[s]?://|www\\.)\\S+')\n",
        "    return url_regex.sub('URL', tweet)\n",
        "\n",
        "  def replace_emojis(self, tweet):\n",
        "    '''Replace emojis by using a pre-defined dictionary containing emojis \n",
        "      along with their meaning. (e.g.: “:)” to “EMOJIsmile”) '''\n",
        "    for emoji in self.emojis.keys():\n",
        "      tweet = tweet.replace(emoji, \"EMOJI\" + self.emojis[emoji]) \n",
        "    return tweet\n",
        "\n",
        "  def replace_username(self, tweet):\n",
        "    ''' Replace @Usernames with the word “USER”. (e.g.: “@Kaggle” to “USER”)'''\n",
        "    user_regex = re.compile(r'@[^\\s]+')\n",
        "    return user_regex.sub('USER', tweet)  \n",
        "\n",
        "  def remove_nonalpha(self, tweet):\n",
        "    ''' Replacing characters except Digits and Alphabets with space.'''\n",
        "    nonalpha_regex = re.compile(r'[^a-zA-Z0-9]')\n",
        "    return nonalpha_regex.sub(\" \", tweet)\n",
        "  \n",
        "  def remove_consecutives(self, tweet):\n",
        "    ''' 3 or more consecutive letters are \n",
        "        replaced by two letters. (e.g.: “Heyyyy” to “Heyy”) '''\n",
        "    sequencePattern   = r\"(.)\\1\\1+\"\n",
        "    seqReplacePattern = r\"\\1\\1\"\n",
        "    return re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "\n",
        "  def remove_stop_short_words(self, tweet):\n",
        "    ''' English words that do not add much meaning to a sentence are removed\n",
        "        and Words with a length of less than two are eliminated.'''\n",
        "    words = nltk.word_tokenize(tweet)\n",
        "    words = [word for word in words if word not in self.stop_words and len(word) >= 2]\n",
        "    return ' '.join(words)\n",
        "\n",
        "  def lemmatize(self, tweet):\n",
        "    ''' Converting word to its base form. '''\n",
        "    tweetwords = ''\n",
        "    for word in tweet.split():\n",
        "      word = WordNetLemmatizer().lemmatize(word)\n",
        "      tweetwords += (word+' ')\n",
        "    return tweetwords\n",
        "\n",
        "  def clean_onetweet(self, tweet):\n",
        "    ''' cleans one single tweet '''\n",
        "    cleaned = self.lowercase(tweet)\n",
        "    cleaned = self.replace_url(cleaned)\n",
        "    cleaned = self.replace_emojis(cleaned)\n",
        "    cleaned = self.replace_username(cleaned)\n",
        "    cleaned = self.remove_nonalpha(cleaned)\n",
        "    cleaned = self.remove_consecutives(cleaned)\n",
        "    cleaned = self.remove_stop_short_words(cleaned)\n",
        "    cleaned = self.lemmatize(cleaned)\n",
        "    return cleaned\n",
        "\n",
        "  def clean_alltweets(self, df):\n",
        "    ''' cleans all tweets in the dataframe'''\n",
        "    df['tweets_processed'] = df['tweet'].apply(self.clean_onetweet)\n",
        "    df = df.drop(columns=['tweet'])\n",
        "    df = df.rename(columns={'tweets_processed': 'tweet'})\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS2oUKEjEjlJ"
      },
      "outputs": [],
      "source": [
        "# testing one tweet\n",
        "tweet = \"@jane and her Dogs have g !$MONEY sitting with https://www.mlq.ai/ai-companies-trading-investing/ babies with feet\"\n",
        "tweetCleaner = TweetCleaner()\n",
        "\n",
        "print(tweetCleaner.clean_onetweet(tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5elH-IxWV6ly"
      },
      "outputs": [],
      "source": [
        "# method for processing tweets\n",
        "def process_tweet_dataframe(df):\n",
        "  tweetCleaner = TweetCleaner()\n",
        "  \n",
        "  t = time.time()\n",
        "  df_processed = tweetCleaner.clean_alltweets(df)\n",
        "  print(f'Text Preprocessing complete.')\n",
        "  print(f'Time Taken: {round(time.time()-t)} seconds')\n",
        "  return df_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OEs1bvDX7LR"
      },
      "outputs": [],
      "source": [
        "df_processed = process_tweet_dataframe(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55h7Ju6SViZC"
      },
      "outputs": [],
      "source": [
        "# view processed dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0_ezaElWYrw"
      },
      "outputs": [],
      "source": [
        "df_processed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvDBnlxZYzJp"
      },
      "source": [
        "### Save processed tweets to csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebM-u2UuYxW0"
      },
      "outputs": [],
      "source": [
        "# save the DataFrame to a CSV file\n",
        "df_processed.to_csv('processed_sentiment140_tweets.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLXDWAL94OPy"
      },
      "source": [
        "### Splitting data into train and test\n",
        "We perform a random split over\n",
        "the dataset to divide the dataset into a training dataset and a testing data set. The training dataset contains 1.52 million tweets, whereas the testing dataset contains 80,000 tweets.\n",
        "\n",
        "5 percent of the training data from the sentiment 140 dataset was used to test the trained models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWJOmxOyZu0o"
      },
      "outputs": [],
      "source": [
        "df_processed.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yZBmnJPmot5"
      },
      "outputs": [],
      "source": [
        "df = df_processed\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"tweet\"], df[\"sentiment\"], test_size=0.05, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuQJQIp6mEdv"
      },
      "source": [
        "### Train the model on SVM\n",
        "\n",
        "The five models used to train were Logistic Regression (LR), Support Vector Machines (SVM), Decision Tree (DT), Boosted Tree (BT), and Random Forests (RF). The **best performance was SVM** (0.83 Accuracy, 0.83 F1 score, 0.83 Precision, 0.83 Recall).\n",
        "\n",
        "The text data is vectorized using TF-IDF (term frequency-inverse document frequency) using the TfidfVectorizer class from scikit-learn. This converts the text data into a numerical feature matrix that can be used to train the SVM model.\n",
        "\n",
        "The SVM model is trained using the SVC class from scikit-learn with a linear kernel and a regularization parameter of 1.0. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1Lu3HMamD6G"
      },
      "outputs": [],
      "source": [
        "# vectorize the text data using TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# train the SVM model\n",
        "svm = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfZSuH_Zr2F7"
      },
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnqFRmpdr52U"
      },
      "outputs": [],
      "source": [
        "# evaluate the SVM model on the testing set\n",
        "y_pred = svm.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
